\section{Emotional Speech Synthesis}

\subsection{Introduction}

As voice-based interfaces become increasingly prevalent, there is a growing demand for Text-to-Speech (TTS) systems that can convey emotions effectively. Emotional speech synthesis aims to go beyond basic intelligibility by creating voices that not only sound natural but also express emotions like happiness, sadness, anger, and calm. Such capabilities enhance human-computer interactions, making them more engaging and relatable. Applications of emotional TTS range from virtual assistants to interactive storytelling and educational tools, where conveying tone and emotion can significantly enrich the user experience.

In this project, we explore the synthesis of emotionally expressive speech using advanced TTS models. Leveraging FastSpeech2, known for its efficient non-autoregressive structure, we aim to achieve high-quality, low-latency speech synthesis. To generate realistic emotional expression, we implement a pipeline that adapts the tone, pitch, and pace of speech in real-time, allowing for diverse emotional outputs. This approach aims to address the challenges of emotional modulation in TTS, especially in maintaining a balance between high fidelity and expressive nuance.

\subsection{State of the Art}

The field of Text-to-Speech (TTS) has evolved significantly, with advancements shifting from basic intelligibility and fluency to the inclusion of emotional and expressive components in synthesized speech. The ability to convey emotion in TTS has become crucial for enhancing user experience across applications, ranging from virtual assistants to interactive storytelling.

1. Early TTS Systems: Concatenative and Parametric Approaches
Traditional TTS systems primarily used concatenative synthesis, assembling pre-recorded audio fragments to create continuous speech. Although intelligible, these methods offered limited flexibility and lacked emotional expressiveness. The development of parametric TTS introduced some improvements by employing statistical models to generate speech waveforms, allowing limited control over pitch, duration, and tone. However, both concatenative and parametric approaches failed to convincingly capture the nuances of human emotional expression [Taylor, 2009].

2. Deep Learning and Neural Network-Based TTS
The shift to neural network-based TTS models marked a transformative phase in speech synthesis. Google’s WaveNet, a groundbreaking autoregressive model, achieved high-quality audio by predicting individual waveform samples. While WaveNet set new standards for naturalness, it was computationally intensive, challenging for real-time applications [van den Oord et al., 2016]. Tacotron models, which introduced sequence-to-sequence architectures, addressed these challenges by first generating spectrograms from text before converting them into audio. This approach improved synthesis quality and allowed for greater expressiveness, though initial models were limited in handling dynamic emotional variation [Wang et al., 2017].

3. FastSpeech and FastSpeech2: Advancing Efficiency and Quality
FastSpeech, followed by FastSpeech2, introduced a non-autoregressive architecture that enabled fast and efficient speech synthesis. FastSpeech2, in particular, offered control over prosodic features like pitch, duration, and energy, making it a practical choice for emotional speech synthesis where expressive modulation is necessary [Ren et al., 2020]. By decoupling the synthesis process from strict autoregressive dependencies, FastSpeech2 achieved low latency and high quality, addressing the need for real-time interaction. However, these models still required additional mechanisms to capture complex emotional cues authentically.

4. Emotion-Driven TTS and Multimodal Encoding
In recent years, research has focused on emotion-driven TTS, leveraging multimodal embeddings and transfer learning to add emotional dimensions to synthesized speech. Emotional Tacotron and variants of FastSpeech have incorporated emotion embeddings to capture and modulate emotions within speech, providing more realistic and contextually relevant outputs [Lorenzo-Trueba et al., 2018; Huang et al., 2021]. This capability to encode emotion dynamically represents a substantial step forward, particularly for applications requiring nuanced emotional expression.

5. HiFi-GAN and High-Fidelity GAN-Based TTS
GAN-based approaches have also emerged, with models like HiFi-GAN providing high-fidelity vocoding by generating high-quality audio from Mel-spectrograms. HiFi-GAN’s ability to handle the nuances of different emotions—by adjusting tonality and pacing while preserving sound quality—has made it a preferred choice for TTS systems requiring expressive, realistic output [Kong et al., 2020]. This advancement has enabled more emotionally resonant TTS systems that are suitable for real-world applications.

6. Contextualized Emotion Synthesis with Conditional VAE
Recently, the Conditional Variational Autoencoder (CVAE) approach has offered new possibilities in contextualized emotion synthesis. The paper “Expressive Speech Synthesis with Style Control and Cross-Speaker Emotion Transfer Using Conditional Variational Autoencoder” [Zhou et al., 2021], presents a robust method for controlling emotional expression through a conditional VAE framework. This model allows for cross-speaker emotion transfer, enabling the synthesis of expressive speech by capturing emotional subtleties and contextual variations within a given style or speaker setting. The CVAE framework combines style control and emotion transfer in a way that is flexible, accurate, and suitable for TTS applications demanding complex emotional variability. This state-of-the-art method provides the foundation for this project, leveraging conditional VAEs to synthesize emotionally expressive, contextually appropriate speech in real-time.

\subsection{Pipeline}

This project’s technical framework combines advanced Text-to-Speech (TTS) models and an emotion-modulation pipeline to generate expressive, realistic speech. FastSpeech2 serves as the backbone for efficient and high-quality synthesis, with additional modules enabling real-time adaptation of speech attributes to convey different emotions.

\subsubseciont{TTS}

The foundation of this TTS system is FastSpeech2, a non-autoregressive model that allows for fast, efficient synthesis while providing control over prosodic features like pitch, duration, and energy. FastSpeech2’s architecture includes a duration predictor, variance adaptors, and multi-head attention layers, all designed to synthesize natural, intelligible speech at high speed [Ren et al., 2020]. The ability to adjust pitch and energy makes FastSpeech2 particularly well-suited for emotional modulation, as it allows fine-grained control over the expressive qualities of speech.

To ensure high-quality audio output, HiFi-GAN is employed as the vocoder, converting the Mel-spectrograms generated by FastSpeech2 into audio waveforms. HiFi-GAN’s GAN-based structure enables high-fidelity synthesis by refining audio quality and capturing subtle acoustic details, crucial for maintaining realism in expressive speech. Together, FastSpeech2 and HiFi-GAN create a powerful synthesis pipeline that balances efficiency with high-quality, emotionally rich audio output.

\subsubsection{Emotional conversio Pipeline}
The emotional speech synthesis pipeline consists of several interconnected modules that work in sequence to produce expressive, realistic speech. This process starts with FastSpeech2, which generates the base speech from text, and then refines it using additional models to encode, modulate, and synthesize emotional nuances in the audio.

The pipeline begins with FastSpeech2, the Text-to-Speech model responsible for creating the initial audio from text input. FastSpeech2 is chosen for its efficient, non-autoregressive architecture, allowing rapid synthesis while providing control over key prosodic features such as pitch, duration, and energy. These features are essential for emotion expression, as they set the foundation for emotional modulation later in the pipeline.

Following the initial TTS synthesis, the pipeline encodes the speech audio into discrete tokens using a HuBERT (Hidden-Unit BERT) model. HuBERT operates as an unsupervised speech representation model that captures both linguistic content and paralinguistic features, including subtle elements of emotion. By transforming the audio into discrete tokens, HuBERT provides a high-level representation that preserves both the emotion and structure of the original speech, enabling the downstream models to manipulate the emotional content effectively. This encoding stage serves as a critical point where the nuances of emotional tone and intent are abstracted into a form that the model can manipulate.

The encoded tokens are then fed into an Encoder-Decoder Transformer, which functions as the emotional modulation layer. This transformer model acts as a translator for emotional expressions, akin to translating between languages. By adjusting the input tokens, the encoder-decoder transformer can shift the emotional tone from one state (such as neutral) to another (such as happy, sad, or angry). This process involves learning a mapping from one emotional representation to another, allowing the model to capture differences in tone, pitch, and inflection that characterize various emotional states. For example, it might intensify pitch variations for happiness or slow down the speech pacing to convey sadness. This transformation step adds a layer of emotional adaptability to the synthesized speech, making it feel more authentic and responsive to context.

Next, two specialized models predict tonality and duration, refining the emotional expression by tailoring the speech’s acoustic qualities to the intended emotion. Tonality prediction adjusts the pitch range and energy of the speech, while duration prediction modulates the pacing. These adjustments align the speech with the natural human tendencies associated with different emotions; for instance, anger might lead to rapid, intense speech, whereas sadness might result in slower, softer speech. By precisely tuning these features, the model can enhance the perceived realism of the emotional expression, bringing it closer to genuine human speech patterns.

Finally, the pipeline reconstructs the audio using HiFi-GAN (High-Fidelity Generative Adversarial Network). HiFi-GAN functions as the vocoder, taking the modified Mel-spectrograms from the transformer and duration-prediction stages and synthesizing them into a high-quality audio waveform. Known for its high fidelity and ability to handle subtle variations in tonality, HiFi-GAN is well-suited for generating natural, expressive speech. This final synthesis step ensures that the emotion-enhanced tokens are rendered into clear, nuanced audio, preserving both the linguistic content and emotional intent. HiFi-GAN’s role is crucial in maintaining the audio’s clarity and expressiveness, particularly for complex emotions where tonal subtleties are important.

All these models were trained on the EmoV dataset, a resource specifically curated for emotional speech synthesis. This dataset includes diverse recordings across multiple emotions, ensuring that the pipeline can generalize well to various emotional expressions while producing realistic audio output. Through this carefully designed and sequential pipeline, the model is able to produce speech that resonates emotionally, adapting its output to the desired tone and enhancing the user’s engagement and experience with the synthesized voice.

Evaluation
To assess the effectiveness of the emotional TTS system, we conducted a user study focused on evaluating key aspects of the synthesized speech: naturalness, emotional accuracy, and overall quality. The study was designed to gauge human perception and satisfaction with the model’s output across different emotions, with ratings provided by a group of participants.

Mean Opinion Score (MOS)
The primary evaluation metric used in this study is the Mean Opinion Score (MOS), a widely accepted subjective measure in TTS research. MOS provides insight into how participants perceive the quality of the audio samples, allowing for a structured, quantitative assessment of attributes like naturalness and emotional clarity. Participants rate each sample on a scale from 1 to 5, with the following scale:

1: Poor
2: Fair
3: Good
4: Very Good
5: Excellent
MOS scores are averaged to yield a mean rating for each evaluated attribute. This scoring system allows for a straightforward comparison across emotions, providing a clear view of the model’s strengths and potential areas for improvement.

User Study Design
The evaluation involved a user study with 10 participants who rated the synthesized speech samples across several criteria. The study included samples representing four distinct emotions—amused, angry, disgusted, and sleepy—each chosen for its unique prosodic and tonal characteristics. The criteria assessed in the study were:

Naturalness
Participants rated the naturalness of each audio sample, indicating how closely the synthesized speech resembled human speech. A high score in naturalness reflects fluid, coherent prosody and clear articulation, essential factors for achieving a realistic, immersive user experience.

Emotional Accuracy
Participants also evaluated the emotional accuracy of each sample by identifying the perceived emotion and then rating how effectively the synthesized voice conveyed the intended emotion. High emotional accuracy indicates that the model successfully captured the distinct tonal and prosodic cues associated with each emotion, an important factor in creating expressive and relatable speech.

Overall Quality
To capture a more general assessment, participants rated the overall quality of each sample, considering both intelligibility and expressiveness. This metric allowed participants to weigh the emotional expressiveness alongside audio quality, providing a comprehensive view of the model’s performance.

Each participant rated a randomized set of synthesized samples across these criteria, ensuring that feedback was consistent and unbiased. The MOS results from this study will provide valuable insights into the effectiveness of the emotional modulation techniques used in this TTS system, helping to identify areas for refinement and future work.

Discussion
The evaluation of the emotional TTS system yielded valuable insights into the model’s strengths and limitations. The Mean Opinion Score (MOS) ratings from the user study showed promising results in both naturalness and emotional accuracy, particularly for emotions with gradual tonal shifts, such as amused and sleepy. The FastSpeech2 and HiFi-GAN combination was effective in producing natural, high-quality speech that participants consistently rated as sounding human-like. This success highlights the robustness of the FastSpeech2 architecture for efficient TTS and the critical role of HiFi-GAN in achieving high fidelity in audio output.

However, the study also revealed challenges with more intense emotions, such as anger and disgust. The encoder-decoder transformer and emotional modulation stages, while effective for steady-state emotions, occasionally smoothed over the abrupt tonal shifts characteristic of high-energy expressions. This smoothing effect reduced the perceived intensity of certain emotions, as reflected in slightly lower MOS ratings for emotional accuracy in angry and disgusted samples. Future work may focus on refining the attention mechanism within the encoder-decoder transformer, potentially incorporating dynamic attention to better capture the rapid tonal shifts required for these complex emotions.

Another key observation was the variability in emotional accuracy across emotions, which suggests that while the system captures general emotional tones effectively, it may benefit from additional data for high-energy emotional states. The study also underscored the importance of balancing tonality and duration adjustments. Although these modifications improved emotional expressiveness for most emotions, extreme adjustments sometimes led to slight unnaturalness, particularly in highly intense emotional expressions. This finding highlights the need for further tuning in the tonality and duration stages, as well as the potential exploration of additional prosodic controls.

Conclusion
This project successfully developed a multi-stage emotional TTS system capable of synthesizing expressive, natural-sounding speech. Through the combination of FastSpeech2, HiFi-GAN, and an emotion modulation pipeline, the system achieved high MOS ratings in both naturalness and emotional accuracy, demonstrating the potential of emotion-driven TTS to enhance human-computer interactions. By enabling synthesized voices that can convey a range of emotions, this work contributes to the growing field of expressive TTS and paves the way for more immersive, engaging applications in virtual assistants, storytelling, and educational tools.

The findings from this project underscore both the feasibility and the challenges of achieving nuanced emotional expression in TTS. While the current system performs well for most emotions, future work will focus on refining its handling of complex, high-energy emotions through enhanced modulation techniques and larger, emotion-diverse datasets. Additionally, expanding the system to support multi-language synthesis and integrating it into real-time, interactive environments represents an exciting avenue for future development. This project thus lays a solid foundation for ongoing advancements in emotional TTS, supporting more meaningful and responsive interactions between humans and AI-driven systems.

Building upon the foundation of expressive speech synthesis, the next phase of research explores the application of emotionally adaptive TTS within an interactive augmented reality setting. This project aims to extend the capabilities of synthesized voices by integrating them into a dynamic AR environment, enabling responsive, lifelike interactions that enhance the immersive experience.
