\section{Emotional Speech Synthesis}

\subsection{Introduction}

As voice-based interfaces become increasingly prevalent, there is a growing demand for Text-to-Speech systems that can convey emotions effectively. Emotional speech synthesis aims to go beyond basic intelligibility by creating voices that not only sound natural but also express emotions like happiness, sadness, anger, and calm. Such capabilities enhance human-computer interactions, making them more engaging and relatable. Applications of emotional TTS range from virtual assistants to interactive storytelling and educational tools, where conveying tone and emotion can significantly enrich the user experience.

In this project, we explore the synthesis of emotionally expressive speech using advanced TTS models. Leveraging FastSpeech2, known for its efficient non-autoregressive structure, we aim to achieve high-quality, low-latency speech synthesis. To generate realistic emotional expression, we implement a pipeline that adapts the tone, pitch, and pace of speech in real-time, allowing for diverse emotional outputs. This approach aims to address the challenges of emotional modulation in TTS, especially in maintaining a balance between high fidelity and expressive nuance.

\subsection{State of the Art}

The field of TTS has evolved significantly, with advancements shifting from basic intelligibility and fluency to the inclusion of emotional and expressive components in synthesized speech. The ability to convey emotion in TTS has become crucial for enhancing user experience across applications, ranging from virtual assistants to interactive storytelling.

\subsection{Evolution of Text-to-Speech Systems: From Concatenation to Neural Networks}

The development of TTS systems has seen remarkable advancements over time, transitioning from basic concatenative methods to sophisticated neural network-based approaches. Initially, TTS systems relied on concatenative synthesis, which generated speech by assembling pre-recorded audio fragments. While these methods were effective in producing intelligible speech, they were inherently rigid and lacked the flexibility to modulate pitch, tone, or emotional expression. This limitation made concatenative systems inadequate for applications that demanded dynamic and natural-sounding speech \cite{taylor2009text}.

To overcome these constraints, parametric TTS systems introduced statistical models that generated speech waveforms from input parameters, providing limited control over prosodic features like pitch, duration, and tone. This shift offered a step forward in flexibility compared to concatenative approaches, but the speech output still lacked the natural variability and emotional richness of human speech. Both concatenative and parametric systems demonstrated the limitations of rule-based and statistical methods, paving the way for a paradigm shift toward data-driven solutions.

The advent of neural network-based TTS marked a turning point in the field, offering unprecedented improvements in naturalness and adaptability. Google’s WaveNet, a groundbreaking autoregressive model, set new benchmarks by synthesizing audio waveforms at the sample level, capturing intricate details of human speech \cite{van2016wavenet}. While WaveNet produced highly natural and expressive speech, its computational intensity and high latency presented significant challenges for real-time applications, making it less practical for interactive systems. This limitation necessitated the exploration of more efficient architectures capable of maintaining quality while reducing latency.

To address these challenges, the introduction of sequence-to-sequence architectures, such as Tacotron and Tacotron 2, provided a more practical solution. These models generated spectrograms from text before converting them into audio using neural vocoders, allowing for smoother and more natural prosody. Tacotron’s ability to capture expressive speech patterns represented a significant improvement over its predecessors. However, its limited capacity for handling dynamic emotional variation underscored the need for further advancements, particularly for real-time adaptability \cite{wang2017tacotron}.

Building on this foundation, the emergence of non-autoregressive architectures such as FastSpeech and FastSpeech2 addressed the remaining bottlenecks in synthesis speed and quality. FastSpeech2 introduced enhanced control over prosodic features, including pitch, duration, and energy, making it particularly suitable for emotional speech synthesis \cite{ren2020fastspeech}. By decoupling the synthesis process from the strict autoregressive dependencies of earlier models, FastSpeech2 achieved low latency without compromising audio quality, thereby meeting the demands of real-time interaction. Despite these advancements, capturing complex and nuanced emotional expressions remains an ongoing challenge, often requiring additional mechanisms to fully replicate human-like emotional responses \cite{liu2021fasttalker}.

\subsubsection{Emotion-Driven TTS and Multimodal Encoding}

The evolution of TTS has expanded beyond generating intelligible and natural-sounding speech to incorporating emotional expressiveness, a crucial aspect for creating engaging and lifelike human-computer interactions. Emotion-driven TTS systems aim to synthesize speech that not only conveys linguistic content but also reflects appropriate emotional tones. Recent advancements in deep learning have enabled significant progress in this domain, leveraging both unimodal and multimodal approaches to improve emotional expressiveness.

Initial efforts in emotion-driven TTS relied on rule-based systems or manually annotated emotional features to modulate pitch, duration, and energy. While these approaches provided some control over expressiveness, they lacked generalization and required extensive manual intervention. Parametric statistical models, such as those based on Hidden Markov Models (HMMs), introduced some flexibility by incorporating emotional labels during training. However, these methods were constrained by their reliance on limited feature sets and inability to capture the subtle dynamics of human emotions\cite{tan2021survey}.

The introduction of neural networks transformed emotion-driven TTS by enabling data-driven learning of emotional cues directly from audio and text. Models like Tacotron and Tacotron 2 laid the groundwork by incorporating prosodic features such as pitch and duration as part of the spectrogram generation process. While these sequence-to-sequence models produced more natural and expressive speech, their ability to handle complex and nuanced emotional variations was limited due to their reliance on fixed embeddings for emotion representation \ciet{shen2018natural}.

To address this limitation, emotional embeddings were introduced, allowing TTS systems to generate speech with varying emotional tones. Emotional Tacotron and its derivatives enhanced emotional expressiveness by using reference encoders to capture prosodic variations from a reference audio input \cite{skerry2018towards}. However, these systems often required high-quality labeled data and lacked flexibility for real-time adaptation.

Other method to create an emotional TTS model is to concat a TTS model and a emotion convertion model. CycleGANs have been applied to emotion conversion to learn mappings between different emotional states in a non-parallel setting. These methods leverage cycle consistency loss to ensure that converted speech retains its original content when transformed back to the source emotion. CycleGAN-based methods have proven effective for basic emotions but struggle with complex emotional transitions and low-resource scenarios \cite{kaneko2019cyclegan}.

To improve the emotion conversion part, the latent space-based methods aim to disentangle emotional features from linguistic and speaker-related information. By separating these aspects in a shared latent space, models can modify emotions independently while preserving other speech characteristics to improve the realism and and the flexibility of the models. Variational Autoencoders (VAEs) and Conditional Variational Autoencoders (CVAEs) are widely used in this context:
\begin{enumerate}
\item VAEs and CVAEs: VAEs have been applied to model latent representations of speech, allowing for smooth interpolation between emotions. CVAEs further extend this approach by conditioning latent variables on emotional labels, enabling explicit control over emotional intensity and type \cite{hsu2018hierarchical}
\item Style Tokens: Techniques like Global Style Tokens (GSTs) have been used to capture emotional and stylistic variations in speech, providing control over these features during synthesis. These methods are particularly useful for applications requiring fine-grained emotional control \cite{kwon2019effective}.
\end{enumerate}

However, VAEs and CVAES rely directly on the text and so the dataset had to cover a wide range of words in all the different emotions for the model to be efficiently trained. Recent advances have focused on discrete representations of speech signals, which decompose speech into separate components for linguistic content, prosody, speaker identity, and emotion. These approaches enable more granular control over emotional expression and have shown promise in addressing non-verbal emotional cues, such as laughter or sighs.\cite{kwon2019effective}.

\subsection{Pipeline}

The various methods for emotional TTS each have their advantages. End-to-end emotional TTS models are relatively straightforward to implement, as they streamline the process into a single model. However, they often lack flexibility in controlling emotional expressions with precision. In contrast, approaches that utilize a modular structure—separating TTS synthesis and emotion conversion—offer greater control over the emotional nuances of the generated speech.

For our application, we will adopt a modular structure to achieve both high-quality synthesis and flexible emotion modulation. The system will begin with a FastSpeech2 model to generate the initial synthesized speech, ensuring efficient and accurate TTS performance. Following this, an emotion voice conversion module will modify the synthesized speech, tailoring it to reflect the desired character’s voice and emotional state. This two-step approach provides the precision and adaptability needed for realistic and contextually appropriate emotional expression in speech.

\subsubsection{Emotional conversion Pipeline}

Following the initial TTS synthesis, the pipeline start by encoding the speech audio into discrete tokens using a HuBERT (Hidden-Unit BERT) model\cite{hsu2021hubert}. HuBERT operates as an unsupervised speech representation model that captures both linguistic content and paralinguistic features, including subtle elements of emotion. By transforming the audio into discrete tokens, HuBERT provides a high-level representation that preserves both the emotion and structure of the original speech, enabling the downstream models to manipulate the emotional content effectively. This encoding stage serves as a critical point where the nuances of emotional tone and intent are abstracted into a form that the translation model for an emotion A to an emotion B can manipulate.

The encoded tokens are then fed into an Encoder-Decoder Transformer. This transformer model acts as a translator for emotional expressions, akin to translating between languages. By adjusting the input tokens, the encoder-decoder transformer can shift the emotional tone from one state (such as neutral) to another (such as happy, sad, or angry). This process involves learning a mapping from one emotional representation to another, allowing the model to capture differences in tone, pitch, and inflection that characterize various emotional states. For example, it might intensify pitch variations for happiness or slow down the speech pacing to convey sadness. This transformation step adds a layer of emotional adaptability to the synthesized speech, making it feel more authentic and responsive to context.

Next, two specialized models predict tonality and duration, refining the emotional expression by tailoring the speech’s acoustic qualities to the intended emotion. Tonality prediction adjusts the pitch range and energy of the speech, while duration prediction modulates the pacing. These adjustments align the speech with the natural human tendencies associated with different emotions; for instance, anger might lead to rapid, intense speech, whereas sadness might result in slower, softer speech. By precisely tuning these features, the model can enhance the perceived realism of the emotional expression, bringing it closer to genuine human speech patterns.

Finally, the pipeline reconstructs the audio using HiFi-GAN (High-Fidelity Generative Adversarial Network)\cite{kong2020hifi}. Usually HiFi-GAN take as input a Mel-Spectrogram and transform it into an audio. This HiFi-GAN is trained differently by taking as input the modified discrete tokens from the emotion translation model, the duration-prediction tokens and synthesizing them into a high-quality audio waveform. Known for its high fidelity and ability to handle subtle variations in tonality, HiFi-GAN is well-suited for generating natural, expressive speech.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{Emotion Conversion pipeline.png}
    \caption{Architecture of the emotion conversion pipeline.}
    \vspace{0.1cm}
    \label{fig:emotion_conversion}
\end{figure}

All these models were trained on the EmoV dataset\cite{adigwe2018emotional}, a resource specifically curated for emotional speech synthesis. This dataset includes diverse recordings across 5 emotions (Neutral, Amused, Angry, Disgusted, Sleepy), ensuring that the pipeline can translate to various emotion for different situation and different speech.

\subsection{Evaluation}
The evaluation involved a user study with 20 participants who rated the synthesized speech samples across several criteria. The study included samples representing five distinct emotions -Neutral, amused, angry, disgusted, and sleepy—each chosen for its unique prosodic and tonal characteristics. The criteria assessed in the study were:

\begin{enumerate}
\item Naturalness: Participants rated the naturalness of each audio sample, indicating how closely the synthesized speech resembled human speech. A high score in naturalness reflects fluid, coherent prosody and clear articulation, essential factors for achieving a realistic, immersive user experience.

\item Emotional Accuracy: Participants also evaluated the emotional accuracy of each sample by identifying the perceived emotion and then rating how effectively the synthesized voice conveyed the intended emotion. High emotional accuracy indicates that the model successfully captured the distinct tonal and prosodic cues associated with each emotion, an important factor in creating expressive and relatable speech.

\item audio Quality: To capture a more general assessment, participants rated the overall quality of each sample, considering both intelligibility and expressiveness. This metric allowed participants to weigh the emotional expressiveness alongside audio quality, providing a comprehensive view of the model’s performance.
\end{enumerate}

Tho evaluate the different criteria, the metric used in this study is the Mean Opinion Score (MOS), a widely accepted subjective measure in TTS research. MOS provides insight into how participants perceive key aspect of the audio. This scoring system allows also a straightforward comparison across emotions, providing a clear view of the model’s strengths and potential areas for improvement. Participants rate each sample on a scale from 1 to 5, with the following scale:

\begin{enumerate}
\item Poor (1/5)
\item Fair (2/5)
\item Good (3/5)
\item Very Good (4/5)
\item Excellent (5/5)
\end{enumerate}

Each participant rated a randomized set of synthesized samples across these criteria, ensuring that feedback was consistent and unbiased. The MOS results from this study will provide valuable insights into the effectiveness of the emotional modulation techniques used in this TTS system, helping to identify areas for refinement and future work.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Evaluation results.png}
    \caption{Combined Mean Opinion Scores for Naturalness, Emotional Accuracy, and Overall Quality across five emotions.}
    \vspace{0.1cm}
    \label{fig:MOS_graph}
\end{figure}

These scores, summarized in the \ref{fig:MOS_graph} above, reveal notable patterns for different emotions. The Neutral emotion, produced by the baseline FastSpeech2 model, serves as a reference point and consistently achieves the highest scores across all criteria. This result aligns with previous research findings, demonstrating the reliability of FastSpeech2 in generating high-quality speech with a natural tone. However, the synthesized emotional speech exhibits disparate results, highlighting the challenges of balancing naturalness, emotional accuracy, and audio quality during emotion conversion.

For the Amused emotion, the speech is perceived as highly unnatural, scoring only 1.3 in naturalness. This low score can be attributed to the synthesized speaker’s exaggerated and continuous laughter, which feels unrealistic and artificial. The emotional accuracy score of 1.5 also reflects this issue, as laughter throughout the audio does not align with how amusement is typically expressed in natural speech. Moreover, the audio quality suffers due to sizzle artifacts introduced during synthesis, resulting in a modest score of 2.4.

In contrast, the Angry emotion fares better across all three criteria. The naturalness score of 3.2 indicates that the speech was perceived as relatively authentic, likely due to the system’s ability to replicate the intensity commonly associated with anger. Emotional accuracy is the highest among all emotions, scoring 3.6, as the tonal shifts and intensity effectively conveyed the intended emotion. Audio quality is also consistent, with a score of 3.6, as no significant distortions or changes in audio length were introduced during synthesis.

The Disgusted emotion reveals some trade-offs between emotional intensity and naturalness. While the emotional accuracy score of 2.9 indicates that the system captured the intensity of disgust reasonably well, the naturalness score of 2.1 suggests that the speech felt less authentic. This lower score can be attributed to a repetitive “disgust gimmick” in the voice, which made the audio feel exaggerated. Audio quality was slightly better, scoring 3.4, as the conversion process did not introduce significant distortions.

For the Sleepy emotion, the scores are more balanced but remain moderate. The naturalness score of 2.4 reflects a reasonable level of authenticity, as the slow and low-pitched voice effectively conveyed a sense of lethargy. Emotional accuracy, with a score of 2.6, indicates that the system adequately captured the characteristics of sleepy speech. However, the audio quality score of 2.8 was impacted by sizzle artifacts, similar to those observed in the Amused emotion.

The Neutral emotion, generated by the baseline FastSpeech2 model, consistently outperformed the emotional outputs, achieving scores of 4.1 in naturalness, 4.0 in emotional accuracy, and 4.6 in audio quality. These results underscore the challenges of introducing emotional modulation while maintaining audio quality and naturalness.

\subsection{Conclusion}

This project has successfully demonstrated the feasibility of a modular pipeline for emotionally expressive TTS synthesis, integrating state-of-the-art technologies such as FastSpeech2, HuBERT, and HiFi-GAN. By decoupling speech synthesis and emotion conversion, the system achieved a balance between efficiency and flexibility, addressing key challenges in emotional TTS.

The baseline Neutral emotion, generated by FastSpeech2, consistently achieved the highest MOS across all evaluation criteria, underscoring its reliability in producing high-quality, natural speech. However, introducing emotional modulation revealed the complexities of balancing naturalness, emotional accuracy, and audio quality. While intense emotions like Angry performed relatively well, nuanced or exaggerated emotions such as Disgusted and Amused exposed limitations in the current approach, including repetitive artifacts and distortions like sizzle, which impacted both naturalness and audio fidelity.

These findings highlight the ongoing challenges of generating realistic and emotionally nuanced speech. Addressing audio artifacts through improved post-processing techniques or enhanced vocoder optimization is a critical next step. Additionally, refining the emotional transformation process to better balance intensity and authenticity will be essential for capturing subtle emotional variations. Expanding training datasets to include diverse linguistic and emotional contexts will further improve the system’s generalization capabilities.

Despite these challenges, the modular pipeline provides a strong foundation for emotionally adaptive TTS systems. Its flexibility makes it suitable for diverse applications, including virtual assistants, interactive storytelling, and educational tools. Future advancements could explore the integration of multimodal inputs, such as visual cues and contextual information, to further enhance emotional expressiveness and adaptability. These improvements will bring TTS systems closer to achieving human-like communication, paving the way for more immersive and engaging human-computer interactions.

This work not only showcases the potential of emotion-driven TTS but also sets the stage for future research to bridge the gap between natural human expression and machine-generated speech. By refining the balance between technical performance and emotional nuance, the next generation of TTS systems can play a transformative role in a wide range of applications, from accessibility tools to creative technologies.
