\section{Emotional Speech Synthesis}

\subsection{Introduction}

As voice-based interfaces become increasingly prevalent, there is a growing demand for Text-to-Speech systems that can convey emotions effectively. Emotional speech synthesis aims to go beyond basic intelligibility by creating voices that not only sound natural but also express emotions like happiness, sadness, anger, and calm. Such capabilities enhance human-computer interactions, making them more engaging and relatable. Applications of emotional TTS range from virtual assistants to interactive storytelling and educational tools, where conveying tone and emotion can significantly enrich the user experience.

In this project, we explore the synthesis of emotionally expressive speech using advanced TTS models. Leveraging FastSpeech2, known for its efficient non-autoregressive structure, we aim to achieve high-quality, low-latency speech synthesis. To generate realistic emotional expression, we implement a pipeline that adapts the tone, pitch, and pace of speech in real-time, allowing for diverse emotional outputs. This approach aims to address the challenges of emotional modulation in TTS, especially in maintaining a balance between high fidelity and expressive nuance.

\subsection{State of the Art}

The field of TTS has evolved significantly, with advancements shifting from basic intelligibility and fluency to the inclusion of emotional and expressive components in synthesized speech. The ability to convey emotion in TTS has become crucial for enhancing user experience across applications, ranging from virtual assistants to interactive storytelling.

\subsection{Evolution of Text-to-Speech Systems: From Concatenation to Neural Networks}

The development of TTS systems has seen remarkable advancements over time, transitioning from basic concatenative methods to sophisticated neural network-based approaches. Initially, TTS systems relied on concatenative synthesis, which generated speech by assembling pre-recorded audio fragments. While these methods were effective in producing intelligible speech, they were inherently rigid and lacked the flexibility to modulate pitch, tone, or emotional expression. This limitation made concatenative systems inadequate for applications that demanded dynamic and natural-sounding speech \cite{taylor2009text}.

To overcome these constraints, parametric TTS systems introduced statistical models that generated speech waveforms from input parameters, providing limited control over prosodic features like pitch, duration, and tone. This shift offered a step forward in flexibility compared to concatenative approaches, but the speech output still lacked the natural variability and emotional richness of human speech. Both concatenative and parametric systems demonstrated the limitations of rule-based and statistical methods, paving the way for a paradigm shift toward data-driven solutions.

The advent of neural network-based TTS marked a turning point in the field, offering unprecedented improvements in naturalness and adaptability. Google’s WaveNet, a groundbreaking autoregressive model, set new benchmarks by synthesizing audio waveforms at the sample level, capturing intricate details of human speech \cite{van2016wavenet}. While WaveNet produced highly natural and expressive speech, its computational intensity and high latency presented significant challenges for real-time applications, making it less practical for interactive systems. This limitation necessitated the exploration of more efficient architectures capable of maintaining quality while reducing latency.

To address these challenges, the introduction of sequence-to-sequence architectures, such as Tacotron and Tacotron 2, provided a more practical solution. These models generated spectrograms from text before converting them into audio using neural vocoders, allowing for smoother and more natural prosody. Tacotron’s ability to capture expressive speech patterns represented a significant improvement over its predecessors. However, its limited capacity for handling dynamic emotional variation underscored the need for further advancements, particularly for real-time adaptability \cite{wang2017tacotron}.

Building on this foundation, the emergence of non-autoregressive architectures such as FastSpeech and FastSpeech2 addressed the remaining bottlenecks in synthesis speed and quality. FastSpeech2 introduced enhanced control over prosodic features, including pitch, duration, and energy, making it particularly suitable for emotional speech synthesis \cite{ren2020fastspeech}. By decoupling the synthesis process from the strict autoregressive dependencies of earlier models, FastSpeech2 achieved low latency without compromising audio quality, thereby meeting the demands of real-time interaction. Despite these advancements, capturing complex and nuanced emotional expressions remains an ongoing challenge, often requiring additional mechanisms to fully replicate human-like emotional responses \cite{liu2021fasttalker}.

\subsubsection{Emotion-Driven TTS and Multimodal Encoding}

The evolution of TTS has expanded beyond generating intelligible and natural-sounding speech to incorporating emotional expressiveness, a crucial aspect for creating engaging and lifelike human-computer interactions. Emotion-driven TTS systems aim to synthesize speech that not only conveys linguistic content but also reflects appropriate emotional tones. Recent advancements in deep learning have enabled significant progress in this domain, leveraging both unimodal and multimodal approaches to improve emotional expressiveness.

Initial efforts in emotion-driven TTS relied on rule-based systems or manually annotated emotional features to modulate pitch, duration, and energy. While these approaches provided some control over expressiveness, they lacked generalization and required extensive manual intervention. Parametric statistical models, such as those based on Hidden Markov Models (HMMs), introduced some flexibility by incorporating emotional labels during training. However, these methods were constrained by their reliance on limited feature sets and inability to capture the subtle dynamics of human emotions\cite{tan2021survey}.

The introduction of neural networks transformed emotion-driven TTS by enabling data-driven learning of emotional cues directly from audio and text. Models like Tacotron and Tacotron 2 laid the groundwork by incorporating prosodic features such as pitch and duration as part of the spectrogram generation process. While these sequence-to-sequence models produced more natural and expressive speech, their ability to handle complex and nuanced emotional variations was limited due to their reliance on fixed embeddings for emotion representation \ciet{shen2018natural}.

To address this limitation, emotional embeddings were introduced, allowing TTS systems to generate speech with varying emotional tones. Emotional Tacotron and its derivatives enhanced emotional expressiveness by using reference encoders to capture prosodic variations from a reference audio input \cite{skerry2018towards}. However, these systems often required high-quality labeled data and lacked flexibility for real-time adaptation.

Other method to create an emotional TTS model is to concat a TTS model and a emotion convertion model. CycleGANs have been applied to emotion conversion to learn mappings between different emotional states in a non-parallel setting. These methods leverage cycle consistency loss to ensure that converted speech retains its original content when transformed back to the source emotion. CycleGAN-based methods have proven effective for basic emotions but struggle with complex emotional transitions and low-resource scenarios \cite{kaneko2019cyclegan}.

To improve the emotion conversion part, the latent space-based methods aim to disentangle emotional features from linguistic and speaker-related information. By separating these aspects in a shared latent space, models can modify emotions independently while preserving other speech characteristics to improve the realism and and the flexibility of the models. Variational Autoencoders (VAEs) and Conditional Variational Autoencoders (CVAEs) are widely used in this context:
\begin{enumerate}
\item VAEs and CVAEs: VAEs have been applied to model latent representations of speech, allowing for smooth interpolation between emotions. CVAEs further extend this approach by conditioning latent variables on emotional labels, enabling explicit control over emotional intensity and type \cite{hsu2018hierarchical}
\item Style Tokens: Techniques like Global Style Tokens (GSTs) have been used to capture emotional and stylistic variations in speech, providing control over these features during synthesis. These methods are particularly useful for applications requiring fine-grained emotional control \cite{kwon2019effective}.
\end{enumerate}

However, VAEs and CVAES rely directly on the text and so the dataset had to cover a wide range of words in all the different emotions for the model to be efficiently trained. Recent advances have focused on discrete representations of speech signals, which decompose speech into separate components for linguistic content, prosody, speaker identity, and emotion. These approaches enable more granular control over emotional expression and have shown promise in addressing non-verbal emotional cues, such as laughter or sighs.\cite{kwon2019effective}.

The differents moethods for emotional TTS has their advantages, Emotional TTS are usually end to end model so easily to make in place but they are 

\subsection{Pipeline}

This project’s technical framework combines advanced Text-to-Speech (TTS) models and an emotion-modulation pipeline to generate expressive, realistic speech. FastSpeech2 serves as the backbone for efficient and high-quality synthesis, with additional modules enabling real-time adaptation of speech attributes to convey different emotions.

\subsubseciont{TTS}

The foundation of this TTS system is FastSpeech2, a non-autoregressive model that allows for fast, efficient synthesis while providing control over prosodic features like pitch, duration, and energy. FastSpeech2’s architecture includes a duration predictor, variance adaptors, and multi-head attention layers, all designed to synthesize natural, intelligible speech at high speed [Ren et al., 2020]. The ability to adjust pitch and energy makes FastSpeech2 particularly well-suited for emotional modulation, as it allows fine-grained control over the expressive qualities of speech.

To ensure high-quality audio output, HiFi-GAN is employed as the vocoder, converting the Mel-spectrograms generated by FastSpeech2 into audio waveforms. HiFi-GAN’s GAN-based structure enables high-fidelity synthesis by refining audio quality and capturing subtle acoustic details, crucial for maintaining realism in expressive speech. Together, FastSpeech2 and HiFi-GAN create a powerful synthesis pipeline that balances efficiency with high-quality, emotionally rich audio output.

\subsubsection{Emotional conversio Pipeline}
The emotional speech synthesis pipeline consists of several interconnected modules that work in sequence to produce expressive, realistic speech. This process starts with FastSpeech2, which generates the base speech from text, and then refines it using additional models to encode, modulate, and synthesize emotional nuances in the audio.

The pipeline begins with FastSpeech2, the Text-to-Speech model responsible for creating the initial audio from text input. FastSpeech2 is chosen for its efficient, non-autoregressive architecture, allowing rapid synthesis while providing control over key prosodic features such as pitch, duration, and energy. These features are essential for emotion expression, as they set the foundation for emotional modulation later in the pipeline.

Following the initial TTS synthesis, the pipeline encodes the speech audio into discrete tokens using a HuBERT (Hidden-Unit BERT) model. HuBERT operates as an unsupervised speech representation model that captures both linguistic content and paralinguistic features, including subtle elements of emotion. By transforming the audio into discrete tokens, HuBERT provides a high-level representation that preserves both the emotion and structure of the original speech, enabling the downstream models to manipulate the emotional content effectively. This encoding stage serves as a critical point where the nuances of emotional tone and intent are abstracted into a form that the model can manipulate.

The encoded tokens are then fed into an Encoder-Decoder Transformer, which functions as the emotional modulation layer. This transformer model acts as a translator for emotional expressions, akin to translating between languages. By adjusting the input tokens, the encoder-decoder transformer can shift the emotional tone from one state (such as neutral) to another (such as happy, sad, or angry). This process involves learning a mapping from one emotional representation to another, allowing the model to capture differences in tone, pitch, and inflection that characterize various emotional states. For example, it might intensify pitch variations for happiness or slow down the speech pacing to convey sadness. This transformation step adds a layer of emotional adaptability to the synthesized speech, making it feel more authentic and responsive to context.

Next, two specialized models predict tonality and duration, refining the emotional expression by tailoring the speech’s acoustic qualities to the intended emotion. Tonality prediction adjusts the pitch range and energy of the speech, while duration prediction modulates the pacing. These adjustments align the speech with the natural human tendencies associated with different emotions; for instance, anger might lead to rapid, intense speech, whereas sadness might result in slower, softer speech. By precisely tuning these features, the model can enhance the perceived realism of the emotional expression, bringing it closer to genuine human speech patterns.

Finally, the pipeline reconstructs the audio using HiFi-GAN (High-Fidelity Generative Adversarial Network). HiFi-GAN functions as the vocoder, taking the modified Mel-spectrograms from the transformer and duration-prediction stages and synthesizing them into a high-quality audio waveform. Known for its high fidelity and ability to handle subtle variations in tonality, HiFi-GAN is well-suited for generating natural, expressive speech. This final synthesis step ensures that the emotion-enhanced tokens are rendered into clear, nuanced audio, preserving both the linguistic content and emotional intent. HiFi-GAN’s role is crucial in maintaining the audio’s clarity and expressiveness, particularly for complex emotions where tonal subtleties are important.

All these models were trained on the EmoV dataset, a resource specifically curated for emotional speech synthesis. This dataset includes diverse recordings across multiple emotions, ensuring that the pipeline can generalize well to various emotional expressions while producing realistic audio output. Through this carefully designed and sequential pipeline, the model is able to produce speech that resonates emotionally, adapting its output to the desired tone and enhancing the user’s engagement and experience with the synthesized voice.

Evaluation
To assess the effectiveness of the emotional TTS system, we conducted a user study focused on evaluating key aspects of the synthesized speech: naturalness, emotional accuracy, and overall quality. The study was designed to gauge human perception and satisfaction with the model’s output across different emotions, with ratings provided by a group of participants.

Mean Opinion Score (MOS)
The primary evaluation metric used in this study is the Mean Opinion Score (MOS), a widely accepted subjective measure in TTS research. MOS provides insight into how participants perceive the quality of the audio samples, allowing for a structured, quantitative assessment of attributes like naturalness and emotional clarity. Participants rate each sample on a scale from 1 to 5, with the following scale:

1: Poor
2: Fair
3: Good
4: Very Good
5: Excellent
MOS scores are averaged to yield a mean rating for each evaluated attribute. This scoring system allows for a straightforward comparison across emotions, providing a clear view of the model’s strengths and potential areas for improvement.

User Study Design
The evaluation involved a user study with 10 participants who rated the synthesized speech samples across several criteria. The study included samples representing four distinct emotions—amused, angry, disgusted, and sleepy—each chosen for its unique prosodic and tonal characteristics. The criteria assessed in the study were:

Naturalness
Participants rated the naturalness of each audio sample, indicating how closely the synthesized speech resembled human speech. A high score in naturalness reflects fluid, coherent prosody and clear articulation, essential factors for achieving a realistic, immersive user experience.

Emotional Accuracy
Participants also evaluated the emotional accuracy of each sample by identifying the perceived emotion and then rating how effectively the synthesized voice conveyed the intended emotion. High emotional accuracy indicates that the model successfully captured the distinct tonal and prosodic cues associated with each emotion, an important factor in creating expressive and relatable speech.

Overall Quality
To capture a more general assessment, participants rated the overall quality of each sample, considering both intelligibility and expressiveness. This metric allowed participants to weigh the emotional expressiveness alongside audio quality, providing a comprehensive view of the model’s performance.

Each participant rated a randomized set of synthesized samples across these criteria, ensuring that feedback was consistent and unbiased. The MOS results from this study will provide valuable insights into the effectiveness of the emotional modulation techniques used in this TTS system, helping to identify areas for refinement and future work.

Discussion
The evaluation of the emotional TTS system yielded valuable insights into the model’s strengths and limitations. The Mean Opinion Score (MOS) ratings from the user study showed promising results in both naturalness and emotional accuracy, particularly for emotions with gradual tonal shifts, such as amused and sleepy. The FastSpeech2 and HiFi-GAN combination was effective in producing natural, high-quality speech that participants consistently rated as sounding human-like. This success highlights the robustness of the FastSpeech2 architecture for efficient TTS and the critical role of HiFi-GAN in achieving high fidelity in audio output.

However, the study also revealed challenges with more intense emotions, such as anger and disgust. The encoder-decoder transformer and emotional modulation stages, while effective for steady-state emotions, occasionally smoothed over the abrupt tonal shifts characteristic of high-energy expressions. This smoothing effect reduced the perceived intensity of certain emotions, as reflected in slightly lower MOS ratings for emotional accuracy in angry and disgusted samples. Future work may focus on refining the attention mechanism within the encoder-decoder transformer, potentially incorporating dynamic attention to better capture the rapid tonal shifts required for these complex emotions.

Another key observation was the variability in emotional accuracy across emotions, which suggests that while the system captures general emotional tones effectively, it may benefit from additional data for high-energy emotional states. The study also underscored the importance of balancing tonality and duration adjustments. Although these modifications improved emotional expressiveness for most emotions, extreme adjustments sometimes led to slight unnaturalness, particularly in highly intense emotional expressions. This finding highlights the need for further tuning in the tonality and duration stages, as well as the potential exploration of additional prosodic controls.

Conclusion
This project successfully developed a multi-stage emotional TTS system capable of synthesizing expressive, natural-sounding speech. Through the combination of FastSpeech2, HiFi-GAN, and an emotion modulation pipeline, the system achieved high MOS ratings in both naturalness and emotional accuracy, demonstrating the potential of emotion-driven TTS to enhance human-computer interactions. By enabling synthesized voices that can convey a range of emotions, this work contributes to the growing field of expressive TTS and paves the way for more immersive, engaging applications in virtual assistants, storytelling, and educational tools.

The findings from this project underscore both the feasibility and the challenges of achieving nuanced emotional expression in TTS. While the current system performs well for most emotions, future work will focus on refining its handling of complex, high-energy emotions through enhanced modulation techniques and larger, emotion-diverse datasets. Additionally, expanding the system to support multi-language synthesis and integrating it into real-time, interactive environments represents an exciting avenue for future development. This project thus lays a solid foundation for ongoing advancements in emotional TTS, supporting more meaningful and responsive interactions between humans and AI-driven systems.

Building upon the foundation of expressive speech synthesis, the next phase of research explores the application of emotionally adaptive TTS within an interactive augmented reality setting. This project aims to extend the capabilities of synthesized voices by integrating them into a dynamic AR environment, enabling responsive, lifelike interactions that enhance the immersive experience.
