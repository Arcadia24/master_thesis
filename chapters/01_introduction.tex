\section{Introduction}

\subsection{Context and overview}

Sound plays a fundamental role in human communication, enriching interactions by conveying not only explicit content but also a range of contextual information. Through voice, we communicate not only the subject matter of a conversation but also convey emotions, tone, and insights about the speaker’s identity—such as age, gender, or mood—through vocal characteristics. Research highlights that human speech transmits an extensive array of paralinguistic information, essential for creating empathy and understanding between speakers \cite{schuller2013paralinguistics}. This combination of explicit and implicit information enhances human interactions, making sound a uniquely powerful communication medium.

In contrast, Human-Computer Interaction (HCI) has traditionally relied on visual and text-based modalities, as images and text provide a straightforward and discreet means of information exchange, particularly in public or shared spaces. However, certain use cases demonstrate that sound-based interaction offers unique advantages. For instance, Augmented Reality (AR) environments benefit from audio's immersive potential, which can deepen user engagement and make digital elements feel more integrated into the real world \cite{yang2022audio}. Additionally, sound-based interaction supports accessibility by providing essential options for visually impaired users or those with limited physical interaction capability \cite{brock2015interactive}.

The potential for sound-based AI in HCI thus holds promise for enhancing and diversifying user experiences. Recent advances in artificial intelligence, particularly with deep learning, have transformed audio applications, allowing for more sophisticated implementations of Text-to-Speech (TTS) \cite{ren2020fastspeech}, Speech-to-Text (STT) \cite{inaguma2020espnet}, and audio classification models \cite{gemmeke2017audio}. These AI models have empowered systems to interpret and generate human speech in ways that were previously unattainable. In particular, generative models now enable dynamic vocal interactions that adapt in real time, moving beyond pre-recorded assets to create more authentic, flexible responses \cite{brown2020language}.

\subsection{Thesis Contributions and Objectives}
This thesis explores various applications of sound-based AI to advance HCI through enriched and immersive experiences. By leveraging recent advancements in AI and sound processing, each project in this work contributes unique insights and practical developments in the field of sound AI.

\begin{enumerate}
\item \textbf{Bird Species Classification by Sound}: The first project serves as both an introduction to sound-based AI and a foundational exploration of audio classification systems. The objective was to build a classification model capable of identifying bird species from their vocalizations, captured through a network of small recording stations deployed in natural environments. This project also included an interface component to visualize results, introducing a User Experience (UX) aspect to sound AI. Through this project, the thesis demonstrates methods for deploying efficient, generalizable models in real-world audio classification tasks, highlighting challenges in sound recognition and generalization.

\item \textbf{Emotional Speech Synthesis}: The second project focuses on generating emotionally expressive synthetic speech, a key capability for creating lifelike and engaging HCI. The objective was to develop a TTS model that could convey specific emotions through tone and intonation, increasing the realism and immersion of spoken interactions. By integrating emotion into synthetic speech, this project pushes the boundaries of audio interaction, allowing machines to respond to human users in a more empathetic and relatable manner.

\item \textbf{Augmented Reality Theatrical Learning Application}: The final project builds on the previous work by applying emotionally expressive TTS within an AR structure for an interactive learning application aimed at theater students. The objective was to synthesize missing voices in theatrical dialogues, creating an immersive rehearsal environment where students could practice alongside virtual counterparts. This AR-based approach enhances the realism and immersion of the learning experience by combining spatial audio with virtual interaction. The project showcases the potential of sound-based AI within AR to create new educational tools, blending AI-driven voice synthesis with interactive learning, and demonstrating the broader possibilities of immersive, voice-based human-computer interaction.
\end{enumerate}
