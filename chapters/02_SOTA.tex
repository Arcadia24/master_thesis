\section{General State of the Art in Sound AI for Human-Machine Interaction}

\subsection{Advances in Sound AI Technologies Through Deep Learning}

The evolution of sound AI has been marked by progressive advancements in deep learning, with each breakthrough building upon previous achievements to enable more nuanced and realistic sound processing. Over the past decade, the journey from early audio processing models to today’s sophisticated transformer architectures has dramatically expanded the capabilities of sound AI in human-computer interaction.

The introduction of Convolutional Neural Networks (CNNs) was pivotal for audio classification tasks, especially in domains requiring complex sound recognition, like environmental monitoring and bioacoustics. CNNs became popular due to their ability to extract relevant features from spectrogram representations of audio, mimicking visual pattern recognition processes in image classification \cite{purwins2019deep}. These early models paved the way for sound-based AI applications by enabling systems to accurately classify audio events in structured datasets, albeit with limitations in handling complex, variable audio environments.

As deep learning progressed, sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks became influential in speech-to-text applications\cite{purwins2019deep}. These models improved on static CNNs by capturing sequential dependencies in audio data, leading to better performance in automatic speech recognition (ASR) tasks, where temporal context is crucial. However, RNNs and LSTMs had scalability issues and often struggled with long sequences, prompting researchers to seek more robust architectures \cite{dong2018speech}.

The development of transformer-based architectures revolutionized sound AI by addressing the limitations of previous models. Originally introduced for natural language processing, transformers eliminated the need for sequential data processing, allowing models to capture global dependencies in audio signals. This shift was particularly impactful in Text-to-Speech, where models like Tacotron and FastSpeech improved the speed and naturalness of synthesized speech \cite{ren2019fastspeech, wang2017tacotron}. FastSpeech2, for instance, adopted a non-autoregressive approach, bypassing the iterative steps of previous models and enabling real-time speech generation suitable for interactive applications \cite{ren2020fastspeech}. 

\subsection{Emerging Applications in AR, VR, and Accessibility}

The advent of generative models in sound AI has opened new possibilities for creating dynamic, real-time vocal interactions essential for immersive applications. Unlike traditional sound processing, which often relies on static, pre-recorded audio assets, generative sound models can produce flexible and context-aware audio responses. This adaptability makes them particularly suited for applications that require personalized or interactive audio, such as virtual assistants, interactive storytelling, and augmented reality environments \cite{1386017}\cite{van2016wavenet}.

Sound-based AI is making significant inroads in fields such as AR, Virtual Reality (VR), and accessibility, where adaptive audio technology can greatly enhance the user experience. In AR and VR environments, sound adds an essential layer of immersion, creating a more holistic and realistic experience by providing auditory cues that align with visual stimuli. Spatial audio, combined with generative sound models, allows for dynamic audio responses that change in real time based on the user’s movements and interactions within the virtual space \cite{su2024sonifyar}.

In addition to enhancing immersion, sound-based AI plays an essential role in improving accessibility. For individuals with visual impairments, audio-based interfaces enable access to digital content, navigation, and communication in ways that are otherwise limited by visual barriers. Text-to-Speech and Speech-to-Text technologies are especially valuable for creating non-visual interfaces, allowing users to interact with systems through spoken language \cite{wald2005enhancing}. Moreover, audio-based feedback in mobile and wearable devices enhances access for users with mobility limitations, enabling hands-free operation and facilitating interaction with smart environments.

These applications highlight sound AI’s versatility and potential to foster inclusivity and immersion in digital interactions. The ongoing advancement of sound-based AI technologies promises even greater accessibility and immersion, opening new possibilities for personalized, engaging, and inclusive experiences in HCI.

