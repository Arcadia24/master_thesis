\section{Bird Species Classification by Sound}

\subsection{Introduction and Objectives}
The classification of bird species based on their vocalizations represents a significant challenge in bioacoustics and sound-based AI. This project aims to develop a sound classification system that can identify bird species from audio recordings, using a setup that captures real-world data from natural environments. As an entry point into sound-based AI, this project provided essential insights into the complexities of audio classification, including the need for robust models capable of handling varied environmental sounds.

The primary objectives of this project were twofold. First, it sought to build an effective classification model trained on a large dataset, specifically the BirdCLEF 2020 dataset, which contains recordings of numerous bird species. This dataset provided a rich source of training data for model development, ensuring the system could generalize across different vocalization patterns. Second, the project aimed to create a simple, user-friendly interface to visualize the classification results. This introduced a user experience component, emphasizing the importance of making complex AI models accessible to users.

\subsection{State of the Art}

\subsubsection{Convolutional Neural Networks in Audio Classification}

Initial approaches in audio classification focused on converting audio recordings into spectrograms, which could then be processed by Convolutional Neural Networks. This method allowed models to capture essential frequency and time-domain patterns in the audio data, which were crucial for classification tasks \cite{zaman2023survey}. Building on this foundation, researchers introduced Mel-spectrograms, which more accurately represent human auditory perception, making them particularly useful in applications where the nuances of sound frequency are important. To enhance the robustness of CNNs, data augmentation techniques—such as adding Gaussian noise or other audio perturbations—were also applied to the spectrograms, effectively improving model generalization in diverse audio environments for animal audio classification \cite{nanni2020data}\cite{sampathkumar2022tuc}.

Then the use of deep CNN architectures as backbones for feature extraction, combined with attention mechanisms to focus on the most relevant parts of the audio input as been highlighted. This integration of attention blocks with CNNs has become the foundation of the SOTA networks, allowing models to identify and prioritize key features with time related context\cite{conde2022few}. Furthermore, to enhance model generalization and reduce vulnerability to adversarial noise, the mixup data augmentation method has been introduced. By combining multiple audio inputs, this approach generates synthetic samples that improve the model’s ability to generalize across diverse scenarios, significantly reducing the risk of adversarial examples \cite{zhang2017mixup}.

The latest advancements in audio classification have introduced significant development by the use of transformer-based models(and not CNN-backbone with Attention Block), which have demonstrated strong performance in handling complex temporal patterns in audio. By leveraging self-attention, transformers can model long-range dependencies within audio data, providing a powerful alternative to traditional CNN and RNN architectures in certain applications \cite{puget2021stft}.

\subsubsection{Applications of Sound AI in Bioacoustics}

Sound AI has become a transformative tool in bioacoustics, enabling researchers to monitor ecosystems and wildlife more effectively. By using audio data from natural environments, these systems allow scientists to capture patterns of animal behavior, track species diversity, and observe ecological changes over time. The applications of sound AI in bioacoustics are varied, extending from conservation efforts to raising public awareness about environmental issues.

One prominent example is the Tidmarsh project, conducted at a 600-acre restored wetland in Southern Massachusetts. Tidmarsh integrates wireless sensors and microphones to monitor wildlife activity in a former industrial cranberry farm, providing valuable insights into the transition from industrial use to protected natural habitat. The project not only supports researchers in studying ecological restoration but also serves an educational purpose by sharing data and soundscapes through a web interface. This public-facing element helps raise awareness about global warming, ecological concerns, and the dynamics of natural ecosystems. However, the complexity of such a system presents challenges; for instance, the technical maintenance of the network can be demanding for researchers, illustrating the need for more accessible, robust AI-powered monitoring solutions \cite{duhart2018deep,duhart2019deep}.

Similarly, Living Sounds is an initiative based in Plymouth, Massachusetts, that broadcasts a live audio feed from a restored wetland. The system mixes sounds from dozens of strategically placed microphones to showcase the richness of natural soundscapes, capturing the activities of animals, plants, insects, and even human interactions with the environment. This project emphasizes the vitality of nature and the complex interactions within ecosystems, fostering a deeper appreciation of wildlife and environmental health. At the same time, it highlights the technical difficulties of implementing a wireless audio system for long-term wildlife monitoring, underscoring the importance of AI in processing and analyzing vast amounts of audio data in real-time \cite{mayton2020sensor}.

These advancements in sound AI and their applications in bioacoustics demonstrate the field’s potential to transform environmental monitoring and species classification. Building on these innovations, this project leverages deep learning architectures and real-time audio processing to classify bird species based on their vocalizations, aiming to contribute to bioacoustic research with a practical, deployable system. The following sections will detail the technical framework, model selection, and implementation steps that enabled the creation of this bird species classifier.

\subsection{Data Collection Module}
The core of the data collection system is a module featuring a sound sensor (INMP441) connected to an ESP32 microcontroller, as shown in Figure \ref{fig:pcb_image}. This module captures environmental sounds and transmits the data wirelessly to an API. To ensure autonomous operation in outdoor conditions, the ESP32 is powered by an 1800 mAh battery delivering 3.85V. For consistent functionality, a voltage regulator is incorporated between the battery and the ESP32 to maintain a stable 3.3V current input. During testing, the module can also be powered by an external generator, ensuring continuous operation. To protect the PCB from weather elements, it is encased in a waterproof wooden box, as illustrated in Figure \ref{fig:pcb_box}. The box is treated with a waterproofing agent, with a hole cut to allow the microphone to remain exposed to capture sound.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{PCB_IRL.jpg}
    \caption{PCB setup with an INMP441 sound sensor connected to an ESP32 microcontroller, powered by a 3.85V, 1800 mAh battery. A voltage regulator ensures a stable 3.3V input to the ESP32 for consistent operation.}
    \vspace{0.1cm}
    \label{fig:pcb_image}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{PCBinbox.jpg}
    \caption{PCB housed in a waterproof box to prevent environmental damage.}
    \vspace{0.1cm}
    \label{fig:pcb_box}
\end{figure}


Each module is programmed to capture 5-second audio clips, which are transmitted using HTTP to a REST API built with FastAPI. To optimize energy efficiency, the ESP32 only sends data when the audio signal surpasses a specified energy threshold, preventing the transmission of empty or irrelevant data and conserving battery life. Additionally, to ensure that the server is aware of each module’s activity status, a secondary program sends a “heartbeat” signal to the server every 10 minutes, confirming that the module is still operational even if no audio data has been sent.

\subsection{Preprocessing and data augmentations}
This modeling and benchmarking process involves firstly a combination of preprocessing, augmentation, and diverse model architectures, all tuned for the task of bird species classification.

The training data for this project is sourced from BirdCLEF\cite{kahl2020overview} and Xeno-Canto\cite{conf/clef/VellingaP15} database, which provides weakly labeled audio recordings with primary and secondary tags for bird species. Following insights from BirdCLEF challenges that indicate longer audio clips can improve model performance, the preprocessing pipeline extracts 30-second audio segments from each recording. These segments are further divided into six 5-second windows to provide more training samples per clip. Each 5-second window is converted to a Mel-spectrogram, with parameters set to a sample rate of 32,000 Hz, 128 Mel bins, a hop size of 512, and a frequency range from 50 to 14,000 Hz. This transformation allows the model to capture frequency and temporal features essential for distinguishing bird vocalizations.

To enhance the model’s robustness and generalization, several data augmentation techniques are applied:

\begin{itemize}
    \item External Noise: Background noise from external datasets like freefield1010 is added to simulate natural conditions \cite{stowell2013open}.
    \item Color Noise: Pink and white noise are generated using the Audiomentation library to introduce realistic variations in background noise.
    \item Tanh Distortion: A distortion effect is added to recordings using the tanh function, which provides a rounded, "soft clipping" distortion.
    \item Low-Pass Filter: High frequencies are reduced to emphasize lower frequencies, which are often more prominent in bird vocalizations.
    \item Mixup: Spectrograms are mixed to create synthetic samples with multiple labels, an augmentation method particularly effective for underrepresented bird species in the dataset. This technique is applied with a probability of 0.7 to prioritize species with limited data\cite{zhang2017mixup}.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{dataaugmentations.png}
    \caption{Example of augmented spectrogram using Gaussian noise and frequency masking, demonstrating enhanced robustness for the model during training.
    }
    \vspace{0.1cm}
    \label{fig:augmentation1}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{mixup.png}
    \caption{Spectrogram with mixup augmentation, blending features from two different audio samples to improve generalization across classes.}
    \vspace{0.1cm}
    \label{fig:augmentation2}
\end{figure}

All the different augmentation are applied with a probability of 0.3 on the different spectrogram during the training part.

The training dataset comprises 80\% of the combined BirdCLEF and Xeno-Canto datasets. To address the inherent class imbalance in the dataset, 70\% of the samples from each class are included in the training set to ensure that no class is underrepresented during training. The remaining data is split into a validation set (20\%) and a test set (10\%). All performance metrics and results are derived exclusively from the test set, ensuring an unbiased evaluation of the model’s ability to generalize to unseen data.

\subsection{Model training}
Multiple architectures are employed during the training phase, each selected for its potential in feature extraction or direct classification:

ResNet, EfficientNet, and EfficientNetV2\cite{he2016deep,tan2019efficientnet,tan2021efficientnetv2} were employed as feature extractors, but since they are not inherently designed to handle temporal patterns, an attention mechanism was added to account for the temporal dynamics of the sound. In contrast, Vision Transformers, Swin Transformers, and Data-Efficient Image Transformers \cite{dosovitskiy2020image, liu2021swin, touvron2021training}inherently capture temporal patterns, eliminating the need for an additional attention block.

During training, the models are optimized using binary cross-entropy loss, with the AdamW optimizer and a cosine annealing scheduler. The initial learning rate is set at 0.001, with a batch size of 64. This setup balances model convergence with computational efficiency, ensuring that the models are capable of learning the intricate patterns in bird vocalizations.

To improve the accuracy for later use, postprocessing involves two thresholding mechanisms to ensure accurate bird call identification and species classification:

\begin{itemize}
\item Call/No-Call Threshold: To determine whether a bird vocalization is present, the maximum confidence score across all species in each 5-second frame is compared against a threshold. If the score falls below this threshold, the segment is classified as "no call."
\item Confidence Threshold: This threshold is parameterized per species, enhancing the reliability of predictions in the presence of diverse acoustic conditions.
\end{itemize}

These postprocessing steps, coupled with the training on the BirdCLEF train soundscape dataset, improve the model's accuracy and reduce false positives, especially in noisy environments.

\subsection{Model results}

\subsubsection{Model Performance Across Different Architectures and Number of Species}

In the initial experiments, each architecture was tested on datasets with an increasing number of species (10, 100, and 397) to evaluate how performance scales with dataset complexity. The results \ref{tab:model_performance} show that as the number of species increases, the classification accuracy decreases due to the similarities in vocalizations among certain bird species. This similarity in acoustic patterns makes it challenging for the models to distinguish between species, leading to a noticeable decline in accuracy, especially when moving from 100 to 400 species.

\caption{Model Performance Across Different Species and Latency (ms)}
\begin{table}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{10 Species} & \textbf{100 Species} & \textbf{397 Species} & \textbf{Latency (ms)} \\
\hline
ResNet-50       & 92.3 & 85.7 & 72.5 & 15.4 \\
EfficientNet-B0 & 94.1 & 88.2 & 74.6 & 12.8 \\
EfficientNetV2  & 95.5 & 90.3 & 78.4 & 11.2 \\
Vision Transformer & 96.2 & 91.8 & 80.7 & 20.5 \\
Swin Transformer   & 96.5 & 92.4 & 81.2 & 18.7 \\
\hline
\end{tabular}
\label{tab:model_performance}
\end{table}

Transformer-based models, such as the Vision Transformer and Swin Transformer, consistently achieve higher accuracy compared to CNN-based models like ResNet and EfficientNet. However, this performance boost comes with increased latency due to the computational demands of transformers. The high latency associated with transformer models can limit their scalability for real-time applications, especially when rapid response times are essential.

\subsubsection{Generalization and Class Imbalance}

While accuracy provides an overall measure of model performance, it can be misleading in datasets with imbalanced classes. Certain rare species, although critically important to recognize, are underrepresented in the training dataset, leading to biased predictions that favor the more common species. This imbalance leads to a model that optimizes for accuracy but may overlook rarer species, where a single misclassification can be significant.

To better evaluate model generalization, we calculate the F0.5 score for each model. The F0.5 score weights false positives more heavily, reflecting the higher cost associated with misclassifying a species presence when it is not detected. The following table compares accuracy and F0.5 scores for each model, showing the disparity between models that perform well on common species versus those that better generalize to all species, including rare ones.
\begin{table}[h] 
\begin{tabular}{|l|c|c|} 
\hline
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{F0.5 Score} \\
\hline 
ResNet-50 & 78.4 & 0.63 \\
EfficientNet-B0 & 81.2 & 0.68 \\
EfficientNetV2 & 83.5 & 0.72 \\
Vision Transformer & 86.3 & 0.75 \\
Swin Transformer & 87.1 & 0.77 \\
\hline 
\end{tabular}
\caption{Model Accuracy and F0.5 Scores for Generalization Performance}
\label{tab:f1_score}
\end{table}

The table highlights that while transformer-based models achieve higher F0.5 scores, indicating better generalization, the CNN-based models show a larger discrepancy between accuracy and F0.5 score. This difference suggests that CNNs may be more prone to overfitting common species, while transformers provide better balance across species.

\subsubsection{Impact of Data Augmentation}

As we present before to improve generalization, data augmentation techniques were applied to the training dataset. By simulating realistic variations in audio, these augmentations help the model better handle variability in real-world conditions.

\begin{table}[h]  
\begin{tabular}{|l|c|c|}{0.6\textwidth} 
\hline 
\textbf{Model} & \textbf{F0.5 Score w/out Aug} & \textbf{F0.5 Score w/ Aug} \\
\hline 
ResNet-50 & 0.61 & 0.68 \\
EfficientNet-B0 & 0.64 & 0.71 \\
EfficientNetV2 & 0.69 & 0.74 \\
Vision Transformer & 0.72 & 0.78 \\
Swin Transformer & 0.74 & 0.80 \\
\hline 
\end{tabular} 
\caption{F0.5 Scores without and with Data Augmentation} 
\label{tab:data_augmentation_results}
\end{table}

The table \ref{data_augmentation_results} above shows the improvement in F0.5 scores for each model when trained with augmentations, demonstrating that augmentation helps the models better handle challenging cases and improves generalization, especially in datasets with class imbalances. This suggests that augmentations are crucial in ensuring that models trained on bioacoustic data can effectively generalize to diverse and underrepresented species.

\subsubsection{general and class-wise thresholds results}

The comparison between the general and class-wise post-processing thresholds reveals notable differences in accuracy across species. The general threshold, represented by a constant red dashed line, applies a single threshold value across all species. This approach provides stable performance, reducing the risk of overfitting, particularly in datasets with variable species representation. However, while the general threshold offers a balanced accuracy level, it may underperform for species with unique vocalization characteristics.

In contrast, the class-wise threshold, illustrated by the blue bars, adjusts the threshold individually for each species. This approach yields improved accuracy for certain species, especially those with distinctive calls, as it tailors the threshold to better capture species-specific patterns. However, this benefit comes at a potential cost: for rarer species with limited data, the class-wise threshold can lead to overfitting, resulting in less generalizable performance. This trade-off highlights the need to balance specificity with robustness in post-processing choices, depending on the diversity and distribution of the species in the dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{ClassWisevsGeneralThreshold.png}
    \caption{Comparison of Class-Wise Thresholding vs. General Thresholding on 10 different species.}
    \vspace{0.1cm}
    \label{fig:vsclass}
\end{figure}

\subsection{Website Interface}
\subsubsection{Interface explanation}

The user interface, developed in Python with the Streamlit framework, serves as an accessible platform for users to interact with the bird classification model. This interface is designed to facilitate easy access to both real-time audio monitoring and previously classified audio, providing a comprehensive tool for both exploration and validation. The system’s data storage and processing, essential for maintaining a responsive interface, are managed on a personal computer with GPU support to handle the computational demands of spectrogram transformations and model inference.

All audio data, along with high-confidence model predictions, are stored on the local file system. This setup not only supports model training and future retraining but also enables efficient access to audio samples for user interaction through the interface. The interface itself is divided into two primary pages, each serving specific functions:

The first page acts as a real-time dashboard where users can view audio input activity from the connected microphones. Displaying incoming audio streams as they are received, this page offers users an immediate sense of the soundscape captured by the microphones, showcasing the live acoustic environment of the forest. Figure\ref{fig:website} shows the interface displaying microphone activity, with the website indicating when no audio has been received (a) and when two audio files have been captured by different modules (b). This page provides users with an immersive auditory experience, allowing them to connect with the forest’s sounds as they occur.

\begin{figure}[h!]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{homepageon.png}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{homepageoff.png}
    \end{minipage}
    \caption{Figure: Comparison of the website’s homepage in inactive (off) and active (on) states. The inactive state (left) indicates no audio received from the modules, while the active state (right) displays the audio streams captured by different modules in real time.}
    \label{fig:website}
\end{figure}


The second page of the interface functions as an audio library where users can review classified recordings along with their corresponding spectrograms, as seen in Figure\ref{fig:website_lib}. Each audio file is labeled with the predicted species, with spectrograms displayed for visual analysis. This page allows users to validate or refine the model’s classifications, contributing to the model’s continuous improvement through active learning. The interface also allows for users to explore and validate the audio in detail, aiding in refining both model performance and user familiarity with different bird vocalizations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.1\textwidth]{librarypage.png}
    \caption{The library page of the website, showcasing stored audio files along with their corresponding spectrograms. Users can listen to the audio, view predictions, and validate or correct the classifications made by the AI for active learning purposes.}
    \vspace{0.1cm}
    \label{fig:website_lib}
\end{figure}

\subsubsection{Performance Evaluation of the Interface}
To ensure a seamless user experience, the page load time was evaluated in two scenarios: initial loading and reloading. In this study, Streamlit’s \textbf{cache data} function was employed to cache data after the first load, significantly reducing subsequent load times. The caching mechanism led to a 33\% decrease in page load time for the second scenario, as illustrated in Figure \ref{fig:reactivity_lib}. This caching feature is essential in maintaining an efficient and responsive interface, particularly when handling large datasets or repeated queries.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{reactivitylibrary.png}
    \caption{ Reactivity analysis of the library page loading times. The comparison highlights the significant reduction in load time achieved using caching mechanisms, improving user experience by ensuring faster access to stored data.}
    \vspace{0.1cm}
    \label{fig:reactivity_lib}
\end{figure}

Evaluating the scalability of the system is crucial for handling increased data loads effectively. The processing time of the API was assessed with different audio lengths (5 and 30 seconds) to determine its responsiveness under varying loads. The system was able to handle these inputs efficiently, with lower processing times for shorter audio segments, supporting real-time interaction without compromising accuracy. This scalability ensures the interface remains responsive even as data loads increase, making it suitable for long-term monitoring and broader deployment.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{reactivityAPI.png}
    \caption{Reactivity analysis of API processing times for audio inputs of different durations.}
    \vspace{0.1cm}
    \label{fig:reactivity_api}
\end{figure}

\subsection{Discussion}
This study demonstrates the potential of sound-based AI in bioacoustic monitoring, specifically for the task of bird species classification in natural environments. By employing deep learning architectures, including CNNs and transformers, we achieved promising accuracy in identifying bird species from audio recordings. However, several limitations emerged during development and evaluation, which suggest directions for future research and improvements.

One significant limitation is the challenge of generalization, particularly for rare species with limited data representation. Despite employing augmentation techniques, the model tended to misclassify these underrepresented classes, underscoring the need for a more balanced dataset. Rare species are often critical to conservation efforts, and thus future data collection efforts should prioritize expanding recordings of these classes. Potential solutions include collaborating with conservation initiatives and exploring crowd-sourced bioacoustic data to build a more comprehensive and balanced training dataset.

Additionally, transformer-based architectures improved classification accuracy but introduced higher latency, posing a constraint for real-time applications. Future research could address this by exploring model optimization techniques, such as quantization and pruning, which could reduce latency without sacrificing accuracy. These optimizations would allow transformer-based models to be more viable for real-time monitoring tasks in field settings.

Another consideration is the system's reliance on GPU resources for spectrogram transformation and model inference. While the personal computer-based setup worked effectively in a controlled environment, deploying such a system in remote locations may prove challenging. Future iterations could focus on developing an optimized version of the model for edge devices, reducing the dependency on high computational power. Such edge deployment would enable autonomous, self-sustaining monitoring stations in diverse and resource-limited environments, enhancing the system’s scalability.

\subsection{Conclusion}
In summary, this research presents a viable approach to bird species classification through the integration of sound-based AI and an intuitive user interface. The system, which incorporates both real-time audio streaming and an audio library with active learning capabilities, offers a user-centered tool that enables users to interact with bioacoustic data, validate predictions, and contribute to model improvements. These features make the system valuable not only for researchers but also for education and public engagement, fostering greater awareness and appreciation of natural soundscapes.

Despite limitations such as class imbalance, latency, and computational constraints, the model achieved promising results, demonstrating that deep learning can enhance our ability to monitor biodiversity in real time. Moving forward, addressing these limitations through enhanced data collection, model optimization, and edge deployment will be key to making sound-based AI more accessible and scalable. This work contributes to the growing field of ecological AI and highlights the potential for sound AI to play a crucial role in conservation and biodiversity monitoring, offering new insights into the dynamics of natural ecosystems.

Building on the foundations of sound-based classification developed in this project, the next phase of research focuses on generating realistic and emotionally expressive speech through Text-to-Speech technology. This project aims to explore how synthesized voices can convey emotions, enhancing human-computer interactions by creating more lifelike and immersive auditory experiences.

