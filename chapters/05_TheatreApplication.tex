\section{Theatre Application}

\subsection{Introduction}

Augmented Reality  and sound-based Human-Computer Interaction have become powerful tools in creating immersive applications. This project explores the integration of these technologies to enhance theater learning, where the sense of immersivity is crucial. Traditional rehearsal methods for theater are often limited by the availability of actors, the challenge of receiving real-time feedback, and the difficulty of maintaining focus without external support. By leveraging AR and sound-based HCI, this project aims to create a platform that addresses these limitations.

The primary objective of this project is to showcase how sound-based HCI can enhance the immersivity of AR applications. The project is set on Second Self, an AR platform developed with the GOSAI framework, designed to create interactive AR experiences. The immersive nature of AR makes it a tangible medium to explore voice-driven interaction compared to conventional devices like smartphones. In this context, theater learning was chosen as a practical use case, where the interactive nature of AR and the emotional expressiveness of sound can significantly elevate the learning experience.

The project emphasizes multi-character scenes to demonstrate the flexibility of AR in creating voices for multiple characters, even when only a single learner is present. This is achieved through pre-produced audio responses integrated into the platform, allowing the learner to engage in realistic dialogues with virtual characters. Additionally, the project addresses the need for real-time feedback, an essential feature for the learner to adjust and improve their performance dynamically. Feedback includes corrections on speech, emotion, and character alignment, all displayed non-intrusively on the AR mirror to avoid disrupting the learning flow.

By integrating voice-driven interaction, learners can select scenes and characters through voice commands, enhancing usability and engagement. The project demonstrates how AR, combined with emotional TTS and real-time processing, can provide a novel approach to learning, paving the way for innovative applications in education and performance training. This effort represents a proof of concept designed to highlight the potential of sound-based HCI and its application in immersive AR platforms.

\subsection{State of the Art}
\subsubsection{AR Applications with Audio}
Audio is a vital element in AR applications, offering users a more immersive and engaging experience by complementing visual feedback with auditory cues. While AR systems often prioritize visual interaction, sound plays a crucial role in delivering spatial awareness, contextual guidance, and emotional depth.

One prominent use of audio in AR is spatial sound. Systems like Microsoft HoloLens\ciet{guo2022hololens} and Google ARCore\cite{oufqir2020arkit} incorporate spatial audio to enhance navigation and interaction. Spatial sound provides directional cues, making virtual objects or interfaces feel more integrated into the physical environment. For example, HoloLens employs 3D sound for virtual meetings, where audio appears to originate from specific points in the virtual space, improving the sense of presence. Similarly, ARCore uses sound effects to guide users through AR-enhanced workflows or games. These implementations demonstrate the potential of audio to anchor users in immersive environments.

Dynamic audio, such as TTS, has further advanced AR applications. TTS is often used in AR-based virtual assistants, delivering instructions or feedback in a human-like voice. However, these implementations are typically limited in expressiveness. Most current systems lack emotional modulation, which is critical for enhancing engagement in scenarios like interactive storytelling or virtual character interactions. Emotional expressiveness remains a significant gap in audio-driven AR systems.

Despite these advancements, audio in AR faces challenges in achieving low latency and synchronization with visual components. For example, in applications where audio guides real-time actions, delays in sound delivery can break immersion and frustrate users. Additionally, current systems tend to treat sound as a secondary feature rather than an integral part of interaction. This project addresses these gaps by incorporating emotion-driven TTS into AR, providing adaptive, expressive audio that enhances dialogue-driven scenarios in theater learning.

\subsubsection{AR Applications in Learning}
AR has emerged as a transformative tool in education, enabling students and professionals to engage in hands-on, immersive learning experiences. Unlike traditional teaching methods, AR provides a medium for experiential and interactive learning, where users can interact directly with virtual elements integrated into their physical environment.

One of the most successful applications of AR in education is the use of 3D models and simulations. Platforms like zSpace\cite{zspace} and Merge Cube\cite{mergecube} allow students to manipulate virtual objects, such as anatomical models or molecular structures, offering a deeper understanding of complex subjects. For instance, zSpace enables interactive dissection of human anatomy, providing a risk-free environment for medical students to explore without the limitations of physical cadavers. These applications showcase how AR promotes better retention and comprehension compared to passive learning methods.

In language learning, AR has also been used to create immersive conversational environments. Applications like MondlyAR\cite{mondlyar} employ virtual avatars to simulate real-world conversations, helping users practice speaking skills in a dynamic and engaging context. The integration of AR avatars bridges the gap between traditional language learning apps and real-world conversational practice, making it easier for learners to gain confidence in their speaking abilities.

Despite these advancements, AR applications have largely overlooked performance-based learning, which requires not only technical knowledge but also emotional and expressive skills. Performance arts, such as theater, demand an immersive environment where learners can practice delivering lines, understanding timing, and conveying emotions. Traditional rehearsal methods are limited by the need for co-actors and real-time feedback, which are not always available\cite{azuma1997survey, billinghurst2015survey}.

This project addresses the gap in AR-based performance learning by integrating emotional TTS with real-time feedback mechanisms. The platform allows learners to rehearse scenes with virtual characters while receiving instant feedback on their speech, emotion, and character alignment. Unlike existing AR education tools that focus on static models or pre-recorded guidance, this project creates a dynamic, interactive environment tailored for theater learning. By leveraging AR and sound-based HCI, it provides an innovative approach to experiential learning, demonstrating the untapped potential of AR in performance training.

\subsection{Application Design and Technical Contributions}
\subsubsection{Platform Overview}

This project is built on the Second Self platform, an AR framework developed using the GOSAI system. Second Self functions as an augmented mirror designed to showcase creative AR applications and explore innovative interactions. While existing applications on this platform primarily rely on visual interfaces or hybrid systems with basic sound feedback, such as the Theremin application, this project introduces a fully voice-driven interface, emphasizing sound-based HCI to enhance immersivity.

The augmented mirror is a compelling medium for this POC because it demonstrates how AR can integrate sound into immersive interactions. Compared to using a smartphone, the mirror's spatial presence and real-time feedback provide a more tangible and engaging experience. The platform supports voice-based commands for play and character selection and real-time textual feedback to guide learners without disrupting their focus during theater rehearsal.

\subsubsection{Technical Contributions}

To enable the system, significant contributions were made to both the GOSAI framework and the Second Self application. These contributions focused on real-time data processing, emotional TTS integration, and modular design for reusability across future projects.

\begin{enumerate}
\item Emotion-Driven TTS Module: Built from the foundation of the second project, this module generates expressive speech using text, emotion, and character parameters. It allows virtual characters in the play to deliver lines with varying emotional tones, enhancing the immersivity of multi-character scenes.
\item Simple TTS Module: This module generates basic speech from text and supports multiple character voices. Designed for general use within GOSAI, it provides quick, non-emotional voice synthesis for applications that do not require emotional modulation.
\end{enumerate}

Both modules are reusable and operate independently within the GOSAI kernel. By leveraging Redis for efficient inter-process communication, these modules can interact seamlessly with other drivers and applications on the platform.

In addition, to address real-time latency challenges, the audio responses of virtual characters were pre-produced during the setup phase. This preprocessing pipeline, conducted independently of the AR environment, ensures that the system can respond almost instantly during interactions. While this approach improved responsiveness, it introduced storage constraints, as the audio had to be prepared for each specific play and character configuration. Moreover, the backend was designed to handleSTT, emotion detection, and character validation in real time, using Python for computational tasks. Although some processes (e.g., STT and emotion detection) caused delays, the modular design allowed for iterative improvements without affecting the core system functionality.

On the other side, while the primary focus was on backend development, the collaboration with Hugo on the frontend ensured a seamless user experience. The frontend, built using JavaScript with the P5.js library, provides a dynamic display for text-based feedback and user interaction.

\subsubsection{Interaction Design}

The system relies entirely on voice-driven interaction for play and character selection. Learners select a play by voicing its name and proceed with their rehearsal using commands like “return” or “continue,” which are displayed at the bottom of the screen for guidance. This approach minimizes the need for manual navigation, creating a more immersive experience.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Image explication app.png}
    \caption{Overview of the AR theater rehearsal application.}
    \vspace{0.1cm}
    \label{fig:app_explication}
\end{figure}

During the rehearsal, learners receive real-time feedback, including:
\begin{enumerate}
\item Speech Correction (STT): Validates spoken lines against the expected script, providing accuracy scores.
\item Emotion Feedback (EMO): Analyzes emotional tone, comparing the learner’s delivery with the intended emotion for the scene.
\item Character Matching (EMB): Confirms whether the correct character delivered the line based on voice characteristics.
\end{enumerate}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{image interface app.png}
    \caption{User interface of the AR theater rehearsal application. This image show the feedback given to the user during the play.}
    \vspace{0.1cm}
    \label{fig:app_interface}
\end{figure}

This feedback is presented on the augmented mirror in a way that avoids disrupting the flow of the rehearsal. For instance, textual feedback appears alongside performance metrics, allowing learners to adjust their delivery dynamically.

\subsection{User Study and Evaluation}

To evaluate the system, a qualitative user study was conducted to assess the overall user experience and the effectiveness of the AR-based rehearsal platform. Participants were invited to rehearse scenes from pre-selected theater plays, interacting with the system to explore its voice-driven features, real-time feedback capabilities, and immersivity. The study focused on how these elements impacted the learner's perception of the platform and their ability to engage with it meaningfully.

Participants interacted with the platform using voice commands for selecting plays, choosing characters, and navigating through the rehearsal process. While commands such as "return" and "continue" were displayed on the screen to guide them, the primary goal was to evaluate the naturalness and intuitiveness of the voice-driven interaction. Feedback from participants indicated that the voice-driven interaction was engaging and novel, with many describing it as "cool." However, some participants noted that they often relied on the visual menu for clarity, suggesting that the interaction could be made more intuitive.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{evalution app.png}
    \caption{User study evaluation results showcasing positive and negative feedback across key categories: "Immersive," "Realistic," and "Intuitive." The graph highlights participant perceptions of the AR theater application.}
    \vspace{0.1cm}
    \label{fig:evalution_app}
\end{figure}

The platform’s ability to provide real-time feedback was a central feature of the study. Participants received corrections on their speech accuracy, emotional delivery, and character alignment, displayed unobtrusively on the mirror. This real-time feedback was valued for its immediacy, as it allowed learners to adjust their performance dynamically during the rehearsal. However, some delays in processing feedback—particularly in speech-to-text (STT) recognition and emotion detection—were noted, which occasionally disrupted the sense of real-time interaction and reduced the immersivity of the experience.

The study also evaluated the audio quality of the voices generated by the platform. Participants consistently praised the neutral voices produced by the baseline FastSpeech2 model for their clarity and naturalness. In contrast, the emotional voices generated by the emotion-driven TTS module received mixed reviews. While participants acknowledged the potential of emotional TTS to enhance immersivity, they noted a lack of realism in the generated voices and the presence of occasional artifacts, such as sizzle. These findings were consistent with observations from the second project, which focused on the development and evaluation of the emotional TTS module.

Overall, the user study highlighted the platform’s strengths in combining AR and sound-based interaction to create an engaging and immersive rehearsal experience. The inclusion of multi-character scenes and real-time feedback allowed learners to practice dynamically, even when rehearsing alone. However, some limitations were also identified. Latency in the feedback system, particularly in STT and emotion detection, impacted the system’s responsiveness, and the emotional TTS voices fell short of providing the realism needed to fully immerse participants in their roles.

Insights from the study provide valuable directions for improvement. Enhancing the emotional TTS module to produce more realistic and expressive voices is a clear priority. Additionally, reducing latency in real-time feedback mechanisms is essential to maintain the sense of immediacy and interaction. Finally, making voice-driven interactions more intuitive could reduce reliance on the visual menu and further elevate the user experience. These findings underline the potential of the platform while highlighting key areas for refinement in future iterations.

\subsection{Challenges and Future Directions}

The development of this AR-based theater rehearsal platform highlighted several challenges that impacted the system’s performance and user experience. These challenges, while significant, also offer clear avenues for improvement and future innovation.

One of the primary challenges was related to latency in the system. While the use of pre-produced audio for TTS significantly improved responsiveness during interactions, other components, such as speech-to-text (STT) recognition and emotion detection, introduced delays. These delays occasionally disrupted the real-time feedback loop, making the system feel less responsive. To address this, future iterations should explore more efficient algorithms or optimized hardware to ensure faster processing. For instance, integrating an end-to-end TTS model capable of real-time synthesis could eliminate the need for pre-produced audio, enhancing both scalability and responsiveness.

Another notable challenge was the quality of the emotional TTS voices. Although participants acknowledged the potential of emotional TTS in creating immersive interactions, the generated voices often lacked realism. Artifacts such as sizzle and exaggerated intonations reduced the believability of the voices, particularly for nuanced emotions. Improving the emotional TTS module is essential to achieving the desired level of immersivity. Advances in neural speech synthesis, such as fine-tuning HiFi-GAN or exploring new approaches like diffusion-based models, could enhance audio fidelity and emotional expressiveness.

The user interface and interaction design also presented opportunities for improvement. While voice-driven interaction was praised as an innovative and engaging feature, participants frequently relied on the visual menu for clarity, indicating that the system could be made more intuitive. Simplifying the voice command structure and providing clearer visual cues could reduce this dependency and create a more seamless interaction experience.

Looking ahead, the platform offers significant potential for scalability and application in other domains. Although currently designed as a proof of concept for theater learning, the framework could be extended to other performance-based or collaborative learning environments. For example, dynamic scene adjustments based on user input could enable improvisation, broadening the platform’s appeal for role-playing games or storytelling applications. Additionally, integrating multimodal inputs, such as gestures or facial expressions, could further enhance the immersivity and realism of the interactions.

Ultimately, this project demonstrates the transformative potential of combining AR and sound-based HCI for immersive applications. By addressing the challenges of latency, emotional TTS quality, and interaction design, future iterations can create even more engaging and realistic experiences. The insights gained from this project serve as a foundation for exploring new possibilities in AR learning and beyond, paving the way for innovative applications that bridge the gap between technology and human creativity.






