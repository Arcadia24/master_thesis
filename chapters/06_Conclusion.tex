\section{Conclusion}
This thesis explored the intersection of sound-based artificial intelligence, text-to-speech (TTS) synthesis, and augmented reality (AR) to develop innovative applications aimed at enhancing human-computer interaction and immersive learning experiences. Each project focused on a unique application area, from bird species classification to emotionally expressive TTS, culminating in the creation of an interactive AR platform for theater training. Together, these projects demonstrate the versatility and potential of sound-based AI to enable meaningful, responsive interactions in diverse settings.

The first project developed a robust classifier for identifying bird species through their vocalizations, addressing challenges in species generalization and optimizing real-time audio processing. By integrating AR with TTS, this work highlighted how sound-based AI can expand our understanding of bioacoustics and support environmental monitoring in an accessible, user-friendly way.

The second project extended these capabilities by synthesizing emotionally adaptive TTS, capable of generating speech with distinct emotional expressions. This system used FastSpeech2 and HiFi-GAN to produce high-quality, emotionally resonant audio, allowing for more lifelike and relatable interactions with virtual agents. The project underscored the potential of TTS to improve human-computer interaction in fields such as virtual assistants and storytelling.

Building on these foundations, the third project implemented an AR-based theater training platform where students could interact with virtual characters capable of dynamic, emotion-driven responses. Through the integration of GOSAI’s AR framework, TTS, and Speech-to-Text (STT) feedback, the system provided a responsive, immersive environment for students to practice essential performance skills. This work represents a significant advancement in educational technology for the performing arts, offering students new ways to engage in practice that closely mirrors real-world conditions.

Across these projects, the challenges of real-time processing, latency optimization, and emotional expressiveness were met with innovative solutions that have broader implications for sound-based AI and AR applications. However, limitations such as achieving high accuracy in complex emotional expressions and the occasional need for further real-time optimization indicate areas for future work. Expanding the range of emotional responses, improving the precision of emotion recognition, and exploring multi-character interactions could further enhance these systems’ capabilities.

This thesis establishes a pathway for integrating sound-based AI into various interactive domains, from bioacoustics to education and performance arts. These contributions underscore the transformative potential of adaptive AI and AR in creating engaging, responsive user experiences, paving the way for further advancements in immersive and accessible technology applications.